{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o28.csv.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"gs\"\r\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\r\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)\r\n\tat scala.collection.immutable.List.map(List.scala:293)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\r\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:537)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8f436f071ea4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local[4]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"air_quality_pyspark\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"delimiter\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gs://air_quality_cyprus/Data_Hourly_2018.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mdf19\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"delimiter\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gs://air_quality_cyprus/Data_Hourly_2019.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mdf20\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"delimiter\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gs://air_quality_cyprus/Data_Hourly_2020.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o28.csv.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"gs\"\r\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\r\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\r\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\r\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\r\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)\r\n\tat scala.collection.immutable.List.map(List.scala:293)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\r\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:537)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n"
     ]
    }
   ],
   "source": [
    "#https://spark.apache.org/docs/latest/sql-data-sources-csv.html\n",
    "#https://sparkbyexamples.com/pyspark/pyspark-convert-array-column-to-string-column/\n",
    "#! /usr/bin/python\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[4]\").appName(\"air_quality_pyspark\").getOrCreate()\n",
    "\n",
    "df = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(\"gs://air_quality_cyprus/Data_Hourly_2018.csv\")\n",
    "df19 = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(\"gs://air_quality_cyprus/Data_Hourly_2019.csv\")\n",
    "df20 = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(\"gs://air_quality_cyprus/Data_Hourly_2020.csv\")\n",
    "df21 = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(\"gs://air_quality_cyprus/Data_Hourly_2021.csv\")\n",
    "df= df.union(df19).union(df20).union(df21)\n",
    "df.show()\n",
    "#df = df.withColumn(\"date_time\",to_timestamp(df.date_time,'dd/MM/yyyy HH:mm'))\n",
    "#df = df.withColumn(\"date\",date_format(df.date_time,'yyyy-MM-dd'))\n",
    "\n",
    "#df = df.withColumn(\"pollutant_value\",df.pollutant_value.cast(DoubleType()))\n",
    "#df_aggr_daily = df.groupby(['date','station_code','pollutant_id']).mean(\"pollutant_value\").withColumnRenamed(\"avg(pollutant_value)\",\"mean_pollutant_value\")\n",
    "\n",
    "#df_aggr_daily.write.option(\"delimiter\", \",\").option(\"header\", True).mode('overwrite').csv('gs://air_quality_cyprus/Data_Daily_All') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+--------------------+\n",
      "|date      |station_code|pollutant_id|mean_pollutant_value|\n",
      "+----------+------------+------------+--------------------+\n",
      "|2018-01-29|1           |45          |2.4291666666666667  |\n",
      "|2018-05-24|1           |45          |0.3041666666666667  |\n",
      "|2018-08-21|1           |45          |0.325               |\n",
      "|2018-11-22|3           |45          |null                |\n",
      "|2018-02-27|8           |45          |0.2916666666666666  |\n",
      "|2018-03-18|8           |45          |0.16250000000000003 |\n",
      "|2018-05-09|8           |45          |null                |\n",
      "|2018-12-08|8           |45          |0.9666666666666668  |\n",
      "|2018-02-12|9           |45          |0.620833333333333   |\n",
      "|2018-06-25|9           |45          |0.27499999999999997 |\n",
      "|2018-09-19|9           |45          |0.29166666666666663 |\n",
      "|2018-11-08|9           |45          |0.5333333333333333  |\n",
      "|2018-06-07|15          |45          |0.26249999999999996 |\n",
      "|2018-12-31|14          |45          |0.21666666666666667 |\n",
      "|2018-02-15|1           |6           |1148.3521739130433  |\n",
      "|2018-05-21|1           |6           |402.9095238095238   |\n",
      "|2018-08-21|1           |6           |439.75652173913056  |\n",
      "|2018-01-08|3           |6           |948.4208333333332   |\n",
      "|2018-01-09|3           |6           |998.7608695652173   |\n",
      "|2018-03-22|3           |6           |640.9772727272727   |\n",
      "+----------+------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "df = df.withColumn(\"date_time\",to_timestamp(df.date_time,'dd/MM/yyyy HH:mm'))\n",
    "df = df.withColumn(\"date\",date_format(df.date_time,'yyyy-MM-dd'))\n",
    "df = df.withColumn(\"pollutant_value\",df.pollutant_value.cast(DoubleType()))\n",
    "df_aggr_daily=  df.groupby(['date','station_code','pollutant_id']).mean(\"pollutant_value\").withColumnRenamed(\"avg(pollutant_value)\",\"mean_pollutant_value\")\n",
    "\n",
    "df_aggr_daily.show(truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|station_code_pk|     station_name_en|\n",
      "+---------------+--------------------+\n",
      "|              1|Nicosia -Traffic ...|\n",
      "|              2|Nicosia - Residen...|\n",
      "|              3|Limassol -Traffic...|\n",
      "|              5|Larnaka - Traffic...|\n",
      "|              8|Zygi -Industrial ...|\n",
      "|              9|EMEP- Ayia Marina...|\n",
      "|             14|Mari - Industrial...|\n",
      "|             15|Pafos - Traffic S...|\n",
      "|             16|Paralimni - Traff...|\n",
      "+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stations = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(\"gs://air_quality_cyprus/AirQualityMonitoringStations.csv\")\n",
    "df_stations = df_stations.select(col(\"station_code\"),col(\"station_name_en\")) #,col(\"latitude\"),col(\"longitude\")\n",
    "df_stations = df_stations.withColumnRenamed('station_code','station_code_pk')\n",
    "df_stations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------------+----------------------+\n",
      "|pollutant_id_pk|pollutant_code|   pollutant_name_en|Unit_of_measurement_en|\n",
      "+---------------+--------------+--------------------+----------------------+\n",
      "|              1|            NO|      Nitrogen Oxide|                 μg/m³|\n",
      "|              2|           NO2|    Nitrogen Dioxide|                 μg/m³|\n",
      "|              3|           NOX|     Nitrogen Oxides|                 μg/m³|\n",
      "|              4|           SO2|      Sulfur Dioxide|                 μg/m³|\n",
      "|              5|            O3|              Ozone |                 μg/m³|\n",
      "|              6|            CO|     Carbon Monoxide|                 μg/m³|\n",
      "|             25|          PM10|Particulate Matte...|                 μg/m³|\n",
      "|             26|         PM2.5|Particulate Matte...|                 μg/m³|\n",
      "|             45|          C6H6|             Benzene|                 μg/m³|\n",
      "+---------------+--------------+--------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Pollutants = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(\"gs://air_quality_cyprus/PollutantsId.csv\")\n",
    "df_Pollutants = df_Pollutants.select(col(\"pollutant_id\"),col(\"pollutant_code\"),col(\"pollutant_name_en\"),col(\"Unit_of_measurement_en\"))\n",
    "df_Pollutants = df_Pollutants.withColumnRenamed('pollutant_id','pollutant_id_pk')\n",
    "# Unit_of_measurement_en:μg/m³\n",
    "df_Pollutants.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+--------------------+--------------------+--------------+-----------------+----------------------+\n",
      "|      date|station_code|pollutant_id|mean_pollutant_value|     station_name_en|pollutant_code|pollutant_name_en|Unit_of_measurement_en|\n",
      "+----------+------------+------------+--------------------+--------------------+--------------+-----------------+----------------------+\n",
      "|2021-08-10|           1|          45|  0.4428034583333334|Nicosia -Traffic ...|          C6H6|          Benzene|                 μg/m³|\n",
      "|2021-01-16|           5|          45|  0.6404166666666667|Larnaka - Traffic...|          C6H6|          Benzene|                 μg/m³|\n",
      "|2021-05-20|           5|          45|  0.5770833333333335|Larnaka - Traffic...|          C6H6|          Benzene|                 μg/m³|\n",
      "|2021-05-28|           8|          45| 0.22624999999999995|Zygi -Industrial ...|          C6H6|          Benzene|                 μg/m³|\n",
      "|2021-08-19|           8|          45| 0.18166666666666667|Zygi -Industrial ...|          C6H6|          Benzene|                 μg/m³|\n",
      "|2021-11-18|           8|          45|               0.595|Zygi -Industrial ...|          C6H6|          Benzene|                 μg/m³|\n",
      "|2021-02-09|           9|          45| 0.47190476190476194|EMEP- Ayia Marina...|          C6H6|          Benzene|                 μg/m³|\n",
      "|2021-04-09|           9|          45|                0.38|EMEP- Ayia Marina...|          C6H6|          Benzene|                 μg/m³|\n",
      "|2021-02-28|          14|          45| 0.07333333333333336|Mari - Industrial...|          C6H6|          Benzene|                 μg/m³|\n",
      "|2021-03-29|           1|           6|   456.0225000000001|Nicosia -Traffic ...|            CO|  Carbon Monoxide|                 μg/m³|\n",
      "|2021-03-03|           3|           6|  421.90500000000003|Limassol -Traffic...|            CO|  Carbon Monoxide|                 μg/m³|\n",
      "|2021-03-05|           3|           6|   293.8408333333333|Limassol -Traffic...|            CO|  Carbon Monoxide|                 μg/m³|\n",
      "|2021-07-14|           3|           6|              463.79|Limassol -Traffic...|            CO|  Carbon Monoxide|                 μg/m³|\n",
      "|2021-11-06|           5|           6|  418.08708333333334|Larnaka - Traffic...|            CO|  Carbon Monoxide|                 μg/m³|\n",
      "|2021-04-10|          14|           6|  141.10909090909092|Mari - Industrial...|            CO|  Carbon Monoxide|                 μg/m³|\n",
      "|2021-11-15|          14|           6|  276.07166666666666|Mari - Industrial...|            CO|  Carbon Monoxide|                 μg/m³|\n",
      "|2021-05-18|          15|           6|  203.64166666666674|Pafos - Traffic S...|            CO|  Carbon Monoxide|                 μg/m³|\n",
      "|2021-12-06|          15|           6|  213.12416666666664|Pafos - Traffic S...|            CO|  Carbon Monoxide|                 μg/m³|\n",
      "|2021-09-13|           1|           2|  29.497916666666665|Nicosia -Traffic ...|           NO2| Nitrogen Dioxide|                 μg/m³|\n",
      "|2021-10-01|           1|           2|  30.804166666666674|Nicosia -Traffic ...|           NO2| Nitrogen Dioxide|                 μg/m³|\n",
      "+----------+------------+------------+--------------------+--------------------+--------------+-----------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Join on aggr data\n",
    "df_aggr_daily = df_aggr_daily.join(df_stations,df_aggr_daily.station_code == df_stations.station_code_pk, how=\"inner\")\n",
    "df_aggr_daily = df_aggr_daily.join(df_Pollutants,df_aggr_daily.pollutant_id == df_Pollutants.pollutant_id_pk, how=\"inner\")\n",
    "df_aggr_daily = df_aggr_daily.drop(\"station_code_pk\").drop(\"pollutant_id_pk\")\n",
    "df_aggr_daily.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>date</th><th>station_code</th><th>pollutant_id</th><th>mean_pollutant_value</th><th>station_name_en</th><th>pollutant_code</th><th>pollutant_name_en</th><th>Unit_of_measurement_en</th></tr>\n",
       "<tr><td>2018-03-14</td><td>3</td><td>4</td><td>1.1375000000000002</td><td>Limassol -Traffic...</td><td>SO2</td><td>Sulfur Dioxide</td><td>μg/m³</td></tr>\n",
       "<tr><td>2018-04-21</td><td>3</td><td>4</td><td>1.629166666666667</td><td>Limassol -Traffic...</td><td>SO2</td><td>Sulfur Dioxide</td><td>μg/m³</td></tr>\n",
       "<tr><td>2018-06-20</td><td>3</td><td>4</td><td>1.3541666666666667</td><td>Limassol -Traffic...</td><td>SO2</td><td>Sulfur Dioxide</td><td>μg/m³</td></tr>\n",
       "<tr><td>2018-10-18</td><td>3</td><td>4</td><td>1.3208333333333335</td><td>Limassol -Traffic...</td><td>SO2</td><td>Sulfur Dioxide</td><td>μg/m³</td></tr>\n",
       "<tr><td>2018-10-30</td><td>3</td><td>4</td><td>3.311111111111111</td><td>Limassol -Traffic...</td><td>SO2</td><td>Sulfur Dioxide</td><td>μg/m³</td></tr>\n",
       "<tr><td>2018-12-15</td><td>3</td><td>4</td><td>1.5291666666666668</td><td>Limassol -Traffic...</td><td>SO2</td><td>Sulfur Dioxide</td><td>μg/m³</td></tr>\n",
       "<tr><td>2018-03-06</td><td>5</td><td>4</td><td>4.529166666666667</td><td>Larnaka - Traffic...</td><td>SO2</td><td>Sulfur Dioxide</td><td>μg/m³</td></tr>\n",
       "<tr><td>2018-06-06</td><td>5</td><td>4</td><td>4.375000000000001</td><td>Larnaka - Traffic...</td><td>SO2</td><td>Sulfur Dioxide</td><td>μg/m³</td></tr>\n",
       "<tr><td>2018-10-01</td><td>8</td><td>4</td><td>0.43750000000000006</td><td>Zygi -Industrial ...</td><td>SO2</td><td>Sulfur Dioxide</td><td>μg/m³</td></tr>\n",
       "<tr><td>2018-11-19</td><td>8</td><td>4</td><td>3.454166666666667</td><td>Zygi -Industrial ...</td><td>SO2</td><td>Sulfur Dioxide</td><td>μg/m³</td></tr>\n",
       "<tr><td>2018-01-29</td><td>9</td><td>4</td><td>0.6090909090909089</td><td>EMEP- Ayia Marina...</td><td>SO2</td><td>Sulfur Dioxide</td><td>μg/m³</td></tr>\n",
       "<tr><td>2018-06-12</td><td>9</td><td>4</td><td>0.40416666666666673</td><td>EMEP- Ayia Marina...</td><td>SO2</td><td>Sulfur Dioxide</td><td>μg/m³</td></tr>\n",
       "<tr><td>2018-01-29</td><td>14</td><td>4</td><td>1.6150000000000002</td><td>Mari - Industrial...</td><td>SO2</td><td>Sulfur Dioxide</td><td>μg/m³</td></tr>\n",
       "<tr><td>2018-08-27</td><td>15</td><td>4</td><td>0.8</td><td>Pafos - Traffic S...</td><td>SO2</td><td>Sulfur Dioxide</td><td>μg/m³</td></tr>\n",
       "<tr><td>2018-12-03</td><td>15</td><td>4</td><td>0.8913043478260868</td><td>Pafos - Traffic S...</td><td>SO2</td><td>Sulfur Dioxide</td><td>μg/m³</td></tr>\n",
       "<tr><td>2018-11-27</td><td>16</td><td>4</td><td>2.558333333333333</td><td>Paralimni - Traff...</td><td>SO2</td><td>Sulfur Dioxide</td><td>μg/m³</td></tr>\n",
       "<tr><td>2018-12-28</td><td>16</td><td>4</td><td>3.0208333333333335</td><td>Paralimni - Traff...</td><td>SO2</td><td>Sulfur Dioxide</td><td>μg/m³</td></tr>\n",
       "<tr><td>2019-02-08</td><td>1</td><td>45</td><td>1.0583333333333333</td><td>Nicosia -Traffic ...</td><td>C6H6</td><td>Benzene</td><td>μg/m³</td></tr>\n",
       "<tr><td>2019-02-18</td><td>5</td><td>45</td><td>1.4541666666666666</td><td>Larnaka - Traffic...</td><td>C6H6</td><td>Benzene</td><td>μg/m³</td></tr>\n",
       "<tr><td>2019-02-27</td><td>5</td><td>45</td><td>0.5208333333333334</td><td>Larnaka - Traffic...</td><td>C6H6</td><td>Benzene</td><td>μg/m³</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+----------+------------+------------+--------------------+--------------------+--------------+-----------------+----------------------+\n",
       "|      date|station_code|pollutant_id|mean_pollutant_value|     station_name_en|pollutant_code|pollutant_name_en|Unit_of_measurement_en|\n",
       "+----------+------------+------------+--------------------+--------------------+--------------+-----------------+----------------------+\n",
       "|2020-11-23|           5|          45|  1.5791666666666668|Larnaka - Traffic...|          C6H6|          Benzene|                 μg/m³|\n",
       "|2020-09-03|          15|          45|  0.7749999999999999|Pafos - Traffic S...|          C6H6|          Benzene|                 μg/m³|\n",
       "|2020-11-30|          15|          45|  0.6529411764705882|Pafos - Traffic S...|          C6H6|          Benzene|                 μg/m³|\n",
       "|2020-01-10|          14|          45|  0.3541666666666667|Mari - Industrial...|          C6H6|          Benzene|                 μg/m³|\n",
       "|2020-09-29|          14|          45|0.008333333333333333|Mari - Industrial...|          C6H6|          Benzene|                 μg/m³|\n",
       "|2020-11-25|           1|           6|   794.5791666666665|Nicosia -Traffic ...|            CO|  Carbon Monoxide|                 μg/m³|\n",
       "|2020-04-26|           2|           6|   184.4333333333333|Nicosia - Residen...|            CO|  Carbon Monoxide|                 μg/m³|\n",
       "|2020-06-02|           2|           6|  185.72727272727278|Nicosia - Residen...|            CO|  Carbon Monoxide|                 μg/m³|\n",
       "|2020-05-14|           3|           6|   494.8208333333334|Limassol -Traffic...|            CO|  Carbon Monoxide|                 μg/m³|\n",
       "|2020-05-31|           3|           6|   385.7826086956521|Limassol -Traffic...|            CO|  Carbon Monoxide|                 μg/m³|\n",
       "|2020-06-16|           3|           6|  271.24583333333334|Limassol -Traffic...|            CO|  Carbon Monoxide|                 μg/m³|\n",
       "|2020-11-05|           5|           6|  291.07391304347817|Larnaka - Traffic...|            CO|  Carbon Monoxide|                 μg/m³|\n",
       "|2020-02-27|           9|           6|                null|EMEP- Ayia Marina...|            CO|  Carbon Monoxide|                 μg/m³|\n",
       "|2020-03-30|           9|           6|                null|EMEP- Ayia Marina...|            CO|  Carbon Monoxide|                 μg/m³|\n",
       "|2020-04-25|           9|           6|  167.60833333333335|EMEP- Ayia Marina...|            CO|  Carbon Monoxide|                 μg/m³|\n",
       "|2020-09-03|           9|           6|  180.60952380952378|EMEP- Ayia Marina...|            CO|  Carbon Monoxide|                 μg/m³|\n",
       "|2020-10-12|           9|           6|            164.3375|EMEP- Ayia Marina...|            CO|  Carbon Monoxide|                 μg/m³|\n",
       "|2020-01-12|          14|           6|  290.44347826086965|Mari - Industrial...|            CO|  Carbon Monoxide|                 μg/m³|\n",
       "|2020-07-21|          14|           6|  244.92499999999998|Mari - Industrial...|            CO|  Carbon Monoxide|                 μg/m³|\n",
       "|2020-01-17|          15|           6|  482.87826086956517|Pafos - Traffic S...|            CO|  Carbon Monoxide|                 μg/m³|\n",
       "+----------+------------+------------+--------------------+--------------------+--------------+-----------------+----------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aggr_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('date', 'string'),\n",
       " ('station_code', 'string'),\n",
       " ('pollutant_id', 'string'),\n",
       " ('mean_pollutant_value', 'double'),\n",
       " ('station_name_en', 'string'),\n",
       " ('pollutant_code', 'string'),\n",
       " ('pollutant_name_en', 'string'),\n",
       " ('Unit_of_measurement_en', 'string')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aggr_daily.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scala code runner version 2.12.10 -- Copyright 2002-2019, LAMP/EPFL and Lightbend, Inc.\r\n"
     ]
    }
   ],
   "source": [
    "!scala -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'version 2.12.10'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext._jvm.scala.util.Properties.versionString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "  .config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar') \\\n",
    "  .getOrCreate()\n",
    "# spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o92.load.\n: java.lang.ClassNotFoundException: Failed to find data source: bigquery. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:678)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:213)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:652)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:652)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:652)\n\t... 13 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-13f57c7469cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# https://github.com/tfayyaz/cloud-dataproc/blob/master/notebooks/python/1.2.%20BigQuery%20Storage%20%26%20Spark%20SQL%20-%20Python.ipynb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"comp548dl-big-data.air_quality_cyprus.daily_data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbigquerydf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bigquery\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"table\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#.option(\"credentialsFile\",\"/Users/rc_user/pyspark_bq_tutorial/my-rcs-project-833123-ef45632b1b12.json\") \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbigquerydf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o92.load.\n: java.lang.ClassNotFoundException: Failed to find data source: bigquery. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:678)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:213)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:652)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:652)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:652)\n\t... 13 more\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/tfayyaz/cloud-dataproc/blob/master/notebooks/python/1.2.%20BigQuery%20Storage%20%26%20Spark%20SQL%20-%20Python.ipynb\n",
    "table = \"comp548dl-big-data.air_quality_cyprus.daily_data\"\n",
    "bigquerydf = spark.read.format(\"bigquery\").option(\"table\", table).load();\n",
    "#.option(\"credentialsFile\",\"/Users/rc_user/pyspark_bq_tutorial/my-rcs-project-833123-ef45632b1b12.json\") \\\n",
    "bigquerydf.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o188.save.\n: java.lang.ClassNotFoundException: Failed to find data source: bigquery. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:678)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:265)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:652)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:652)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:652)\n\t... 12 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4926ddb12f90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Saving the data to BigQuery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_aggr_daily\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bigquery'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'comp548dl-big-data.air_quality_cyprus.daily_data_v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o188.save.\n: java.lang.ClassNotFoundException: Failed to find data source: bigquery. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:678)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:265)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:652)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:652)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:652)\n\t... 12 more\n"
     ]
    }
   ],
   "source": [
    "# Saving the data to BigQuery\n",
    "df_aggr_daily.write.format('bigquery') \\\n",
    "  .option('table', 'comp548dl-big-data.air_quality_cyprus.daily_data_v2') \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>station_code</th>\n",
       "      <th>pollutant_id</th>\n",
       "      <th>mean_pollutant_value</th>\n",
       "      <th>station_name_en</th>\n",
       "      <th>pollutant_code</th>\n",
       "      <th>pollutant_name_en</th>\n",
       "      <th>Unit_of_measurement_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-20</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>2.020833</td>\n",
       "      <td>Nicosia -Traffic Station</td>\n",
       "      <td>C6H6</td>\n",
       "      <td>Benzene</td>\n",
       "      <td>μg/m³</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-24</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0.906571</td>\n",
       "      <td>Nicosia -Traffic Station</td>\n",
       "      <td>C6H6</td>\n",
       "      <td>Benzene</td>\n",
       "      <td>μg/m³</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-06-25</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0.279167</td>\n",
       "      <td>Nicosia -Traffic Station</td>\n",
       "      <td>C6H6</td>\n",
       "      <td>Benzene</td>\n",
       "      <td>μg/m³</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-26</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Nicosia -Traffic Station</td>\n",
       "      <td>C6H6</td>\n",
       "      <td>Benzene</td>\n",
       "      <td>μg/m³</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-08-09</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>Nicosia -Traffic Station</td>\n",
       "      <td>C6H6</td>\n",
       "      <td>Benzene</td>\n",
       "      <td>μg/m³</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-10-05</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>Nicosia -Traffic Station</td>\n",
       "      <td>C6H6</td>\n",
       "      <td>Benzene</td>\n",
       "      <td>μg/m³</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-11-25</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>Nicosia -Traffic Station</td>\n",
       "      <td>C6H6</td>\n",
       "      <td>Benzene</td>\n",
       "      <td>μg/m³</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-09-19</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0.471328</td>\n",
       "      <td>Nicosia -Traffic Station</td>\n",
       "      <td>C6H6</td>\n",
       "      <td>Benzene</td>\n",
       "      <td>μg/m³</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-05-31</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0.279617</td>\n",
       "      <td>Nicosia -Traffic Station</td>\n",
       "      <td>C6H6</td>\n",
       "      <td>Benzene</td>\n",
       "      <td>μg/m³</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-02-10</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>Nicosia -Traffic Station</td>\n",
       "      <td>C6H6</td>\n",
       "      <td>Benzene</td>\n",
       "      <td>μg/m³</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  station_code  pollutant_id  mean_pollutant_value  \\\n",
       "0  2018-11-20             1            45              2.020833   \n",
       "1  2021-02-24             1            45              0.906571   \n",
       "2  2018-06-25             1            45              0.279167   \n",
       "3  2019-10-26             1            45              1.000000   \n",
       "4  2019-08-09             1            45              0.395833   \n",
       "5  2020-10-05             1            45              0.920833   \n",
       "6  2020-11-25             1            45              2.250000   \n",
       "7  2021-09-19             1            45              0.471328   \n",
       "8  2021-05-31             1            45              0.279617   \n",
       "9  2018-02-10             1            45              0.883333   \n",
       "\n",
       "            station_name_en pollutant_code pollutant_name_en  \\\n",
       "0  Nicosia -Traffic Station           C6H6           Benzene   \n",
       "1  Nicosia -Traffic Station           C6H6           Benzene   \n",
       "2  Nicosia -Traffic Station           C6H6           Benzene   \n",
       "3  Nicosia -Traffic Station           C6H6           Benzene   \n",
       "4  Nicosia -Traffic Station           C6H6           Benzene   \n",
       "5  Nicosia -Traffic Station           C6H6           Benzene   \n",
       "6  Nicosia -Traffic Station           C6H6           Benzene   \n",
       "7  Nicosia -Traffic Station           C6H6           Benzene   \n",
       "8  Nicosia -Traffic Station           C6H6           Benzene   \n",
       "9  Nicosia -Traffic Station           C6H6           Benzene   \n",
       "\n",
       "  Unit_of_measurement_en  \n",
       "0                  μg/m³  \n",
       "1                  μg/m³  \n",
       "2                  μg/m³  \n",
       "3                  μg/m³  \n",
       "4                  μg/m³  \n",
       "5                  μg/m³  \n",
       "6                  μg/m³  \n",
       "7                  μg/m³  \n",
       "8                  μg/m³  \n",
       "9                  μg/m³  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "SELECT *\n",
    "FROM comp548dl-big-data.air_quality_cyprus.daily_data\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StoreData\n",
    "df_aggr_daily.write.option(\"delimiter\", \",\")\\\n",
    " .option(\"quote\", \"\\\"\").option(\"header\", True)\\\n",
    " .mode('overwrite')\\\n",
    " .csv('gs://air_quality_cyprus/Data_Daily_All')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames can be saved as Parquet files, maintaining the schema information.\n",
    "df_aggr_daily.write.parquet(\"data.parquet\")\n",
    "\n",
    "# Read in the Parquet file created above.\n",
    "# Parquet files are self-describing so the schema is preserved.\n",
    "# The result of loading a parquet file is also a DataFrame.\n",
    "parquetFile = spark.read.parquet(\"data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- station_code: string (nullable = true)\n",
      " |-- pollutant_id: string (nullable = true)\n",
      " |-- mean_pollutant_value: double (nullable = true)\n",
      " |-- station_name_en: string (nullable = true)\n",
      " |-- pollutant_code: string (nullable = true)\n",
      " |-- pollutant_name_en: string (nullable = true)\n",
      " |-- Unit_of_measurement_en: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Table or view not found: parquetFile; line 1 pos 66'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o54.sql.\n: org.apache.spark.sql.AnalysisException: Table or view not found: parquetFile; line 1 pos 66\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:798)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:750)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$6.applyOrElse(Analyzer.scala:780)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$6.applyOrElse(Analyzer.scala:773)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:773)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:719)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:87)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:643)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'parquetfile' not found in database 'default';\n\tat org.apache.spark.sql.hive.client.HiveClient.$anonfun$getTable$1(HiveClient.scala:81)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.hive.client.HiveClient.getTable(HiveClient.scala:81)\n\tat org.apache.spark.sql.hive.client.HiveClient.getTable$(HiveClient.scala:80)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:85)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:118)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$getTable$1(HiveExternalCatalog.scala:700)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:700)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:795)\n\t... 50 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-266ca250b3bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Parquet files can also be used to create a temporary view and then used in SQL statements.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#parquetFile.createOrReplaceTempView(\"parquetFile\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstation_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT pollutant_id,station_code, avg(mean_pollutant_value)  FROM parquetFile Group by pollutant_id,station_code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mstation_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Table or view not found: parquetFile; line 1 pos 66'"
     ]
    }
   ],
   "source": [
    "# Parquet files can also be used to create a temporary view and then used in SQL statements.\n",
    "parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "station_values = spark.sql(\"SELECT pollutant_id,station_code, avg(mean_pollutant_value)  FROM parquetFile Group by pollutant_id,station_code\")\n",
    "station_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+------------+---------------+-------------------+----+\n",
      "|date_time       |station_code|pollutant_id|pollutant_value|date_type          |hour|\n",
      "+----------------+------------+------------+---------------+-------------------+----+\n",
      "|01/01/2018 0:00 |1           |45          |4.8            |2018-01-01 00:00:00|00  |\n",
      "|01/01/2018 1:00 |1           |45          |5.3            |2018-01-01 01:00:00|01  |\n",
      "|01/01/2018 2:00 |1           |45          |6.1            |2018-01-01 02:00:00|02  |\n",
      "|01/01/2018 3:00 |1           |45          |4.0            |2018-01-01 03:00:00|03  |\n",
      "|01/01/2018 4:00 |1           |45          |3.7            |2018-01-01 04:00:00|04  |\n",
      "|01/01/2018 5:00 |1           |45          |1.2            |2018-01-01 05:00:00|05  |\n",
      "|01/01/2018 6:00 |1           |45          |1.0            |2018-01-01 06:00:00|06  |\n",
      "|01/01/2018 7:00 |1           |45          |1.1            |2018-01-01 07:00:00|07  |\n",
      "|01/01/2018 8:00 |1           |45          |1.9            |2018-01-01 08:00:00|08  |\n",
      "|01/01/2018 9:00 |1           |45          |0.9            |2018-01-01 09:00:00|09  |\n",
      "|01/01/2018 10:00|1           |45          |0.4            |2018-01-01 10:00:00|10  |\n",
      "|01/01/2018 11:00|1           |45          |0.3            |2018-01-01 11:00:00|11  |\n",
      "|01/01/2018 12:00|1           |45          |0.5            |2018-01-01 12:00:00|12  |\n",
      "|01/01/2018 13:00|1           |45          |0.3            |2018-01-01 13:00:00|13  |\n",
      "|01/01/2018 14:00|1           |45          |0.3            |2018-01-01 14:00:00|14  |\n",
      "|01/01/2018 15:00|1           |45          |0.4            |2018-01-01 15:00:00|15  |\n",
      "|01/01/2018 16:00|1           |45          |0.3            |2018-01-01 16:00:00|16  |\n",
      "|01/01/2018 17:00|1           |45          |0.3            |2018-01-01 17:00:00|17  |\n",
      "|01/01/2018 18:00|1           |45          |0.3            |2018-01-01 18:00:00|18  |\n",
      "|01/01/2018 19:00|1           |45          |0.6            |2018-01-01 19:00:00|19  |\n",
      "+----------------+------------+------------+---------------+-------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#union https://stackoverflow.com/questions/37332434/concatenate-two-pyspark-dataframes\n",
    "#https://sparkbyexamples.com/pyspark/pyspark-to_date-convert-timestamp-to-date/#:~:text=PySpark%20timestamp%20(%20TimestampType%20)%20consists%20of,to%20date%20on%20DataFrame%20column.\n",
    "#https://sparkbyexamples.com/pyspark/pyspark-date_format-convert-date-to-string-format/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, station_code: string, pollutant_id: string, mean_pollutant_value: string]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_readback = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(\"gs://air_quality_cyprus/Data_Daily_All\")\n",
    "df_readback                                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+--------------------+\n",
      "|      date|station_code|pollutant_id|mean_pollutant_value|\n",
      "+----------+------------+------------+--------------------+\n",
      "|2018-03-11|           1|          45|  0.8571428571428571|\n",
      "|2018-01-28|           3|          45|   2.895454545454545|\n",
      "|2018-02-01|           3|          45|                null|\n",
      "|2018-06-30|           3|          45| 0.40833333333333327|\n",
      "|2018-08-09|           3|          45|                null|\n",
      "|2018-10-09|           3|          45|                null|\n",
      "|2018-12-02|           3|          45|                null|\n",
      "|2018-12-11|           3|          45|                null|\n",
      "|2018-01-24|           5|          45|  1.6124999999999998|\n",
      "|2018-02-01|           9|          45|  0.7041666666666665|\n",
      "|2018-09-14|          15|          45| 0.25833333333333325|\n",
      "|2018-10-02|          15|          45| 0.36956521739130443|\n",
      "|2018-02-03|          14|          45|                null|\n",
      "|2018-02-22|          14|          45|                null|\n",
      "|2018-02-07|           1|           6|   887.5173913043479|\n",
      "|2018-08-02|           2|           6|  331.42916666666673|\n",
      "|2018-08-03|           2|           6|  295.68750000000006|\n",
      "|2018-11-23|           2|           6|  450.75000000000006|\n",
      "|2018-03-31|           3|           6|            358.6375|\n",
      "|2018-01-26|           5|           6|  479.16666666666674|\n",
      "+----------+------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_readback.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load to Pandas for flexility and play around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df_aggr_daily  = df_aggr_daily.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                    0\n",
       "station_code            0\n",
       "pollutant_id            0\n",
       "mean_pollutant_value    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df_aggr_daily[pd_df_aggr_daily.isna()==True].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1086da0d10>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df_aggr_daily.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
