{"cells": [{"cell_type": "code", "execution_count": 74, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------+------------+------------+---------------+\n|       date_time|station_code|pollutant_id|pollutant_value|\n+----------------+------------+------------+---------------+\n| 01/01/2018 0:00|           1|          45|            4.8|\n| 01/01/2018 1:00|           1|          45|            5.3|\n| 01/01/2018 2:00|           1|          45|            6.1|\n| 01/01/2018 3:00|           1|          45|            4.0|\n| 01/01/2018 4:00|           1|          45|            3.7|\n| 01/01/2018 5:00|           1|          45|            1.2|\n| 01/01/2018 6:00|           1|          45|            1.0|\n| 01/01/2018 7:00|           1|          45|            1.1|\n| 01/01/2018 8:00|           1|          45|            1.9|\n| 01/01/2018 9:00|           1|          45|            0.9|\n|01/01/2018 10:00|           1|          45|            0.4|\n|01/01/2018 11:00|           1|          45|            0.3|\n|01/01/2018 12:00|           1|          45|            0.5|\n|01/01/2018 13:00|           1|          45|            0.3|\n|01/01/2018 14:00|           1|          45|            0.3|\n|01/01/2018 15:00|           1|          45|            0.4|\n|01/01/2018 16:00|           1|          45|            0.3|\n|01/01/2018 17:00|           1|          45|            0.3|\n|01/01/2018 18:00|           1|          45|            0.3|\n|01/01/2018 19:00|           1|          45|            0.6|\n+----------------+------------+------------+---------------+\nonly showing top 20 rows\n\n"}], "source": "#https://spark.apache.org/docs/latest/sql-data-sources-csv.html\n#https://sparkbyexamples.com/pyspark/pyspark-convert-array-column-to-string-column/\n#! /usr/bin/python\nimport pyspark\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import DoubleType\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[4]\").appName(\"air_quality_pyspark\").getOrCreate()\n\ndf = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(\"gs://air_quality_cyprus/Data_Hourly_2018.csv\")\ndf19 = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(\"gs://air_quality_cyprus/Data_Hourly_2019.csv\")\ndf20 = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(\"gs://air_quality_cyprus/Data_Hourly_2020.csv\")\ndf21 = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(\"gs://air_quality_cyprus/Data_Hourly_2021.csv\")\ndf= df.union(df19).union(df20).union(df21)\ndf.show()\n#df = df.withColumn(\"date_time\",to_timestamp(df.date_time,'dd/MM/yyyy HH:mm'))\n#df = df.withColumn(\"date\",date_format(df.date_time,'yyyy-MM-dd'))\n\n#df = df.withColumn(\"pollutant_value\",df.pollutant_value.cast(DoubleType()))\n#df_aggr_daily = df.groupby(['date','station_code','pollutant_id']).mean(\"pollutant_value\").withColumnRenamed(\"avg(pollutant_value)\",\"mean_pollutant_value\")\n\n#df_aggr_daily.write.option(\"delimiter\", \",\").option(\"header\", True).mode('overwrite').csv('gs://air_quality_cyprus/Data_Daily_All') \n"}, {"cell_type": "code", "execution_count": 133, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+------------+------------+--------------------+\n|date      |station_code|pollutant_id|mean_pollutant_value|\n+----------+------------+------------+--------------------+\n|2018-01-29|1           |45          |2.4291666666666667  |\n|2018-05-24|1           |45          |0.3041666666666667  |\n|2018-08-21|1           |45          |0.325               |\n|2018-11-22|3           |45          |null                |\n|2018-02-27|8           |45          |0.2916666666666666  |\n|2018-03-18|8           |45          |0.16250000000000003 |\n|2018-05-09|8           |45          |null                |\n|2018-12-08|8           |45          |0.9666666666666668  |\n|2018-02-12|9           |45          |0.620833333333333   |\n|2018-06-25|9           |45          |0.27499999999999997 |\n|2018-09-19|9           |45          |0.29166666666666663 |\n|2018-11-08|9           |45          |0.5333333333333333  |\n|2018-06-07|15          |45          |0.26249999999999996 |\n|2018-12-31|14          |45          |0.21666666666666667 |\n|2018-02-15|1           |6           |1148.3521739130433  |\n|2018-05-21|1           |6           |402.9095238095238   |\n|2018-08-21|1           |6           |439.75652173913056  |\n|2018-01-08|3           |6           |948.4208333333332   |\n|2018-01-09|3           |6           |998.7608695652173   |\n|2018-03-22|3           |6           |640.9772727272727   |\n+----------+------------+------------+--------------------+\nonly showing top 20 rows\n\n"}], "source": "from pyspark.sql.functions import *\nfrom pyspark.sql.types import DoubleType\n\ndf = df.withColumn(\"date_time\",to_timestamp(df.date_time,'dd/MM/yyyy HH:mm'))\ndf = df.withColumn(\"date\",date_format(df.date_time,'yyyy-MM-dd'))\ndf = df.withColumn(\"pollutant_value\",df.pollutant_value.cast(DoubleType()))\ndf_aggr_daily=  df.groupby(['date','station_code','pollutant_id']).mean(\"pollutant_value\").withColumnRenamed(\"avg(pollutant_value)\",\"mean_pollutant_value\")\n\ndf_aggr_daily.show(truncate=False) "}, {"cell_type": "code", "execution_count": 132, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------------+--------------------+\n|station_code_pk|     station_name_en|\n+---------------+--------------------+\n|              1|Nicosia -Traffic ...|\n|              2|Nicosia - Residen...|\n|              3|Limassol -Traffic...|\n|              5|Larnaka - Traffic...|\n|              8|Zygi -Industrial ...|\n|              9|EMEP- Ayia Marina...|\n|             14|Mari - Industrial...|\n|             15|Pafos - Traffic S...|\n|             16|Paralimni - Traff...|\n+---------------+--------------------+\n\n"}], "source": "df_stations = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(\"gs://air_quality_cyprus/AirQualityMonitoringStations.csv\")\ndf_stations = df_stations.select(col(\"station_code\"),col(\"station_name_en\")) #,col(\"latitude\"),col(\"longitude\")\ndf_stations = df_stations.withColumnRenamed('station_code','station_code_pk')\ndf_stations.show()"}, {"cell_type": "code", "execution_count": 126, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------------+--------------+--------------------+----------------------+\n|pollutant_id_pk|pollutant_code|   pollutant_name_en|Unit_of_measurement_en|\n+---------------+--------------+--------------------+----------------------+\n|              1|            NO|      Nitrogen Oxide|                 \u03bcg/m\u00b3|\n|              2|           NO2|    Nitrogen Dioxide|                 \u03bcg/m\u00b3|\n|              3|           NOX|     Nitrogen Oxides|                 \u03bcg/m\u00b3|\n|              4|           SO2|      Sulfur Dioxide|                 \u03bcg/m\u00b3|\n|              5|            O3|              Ozone\u00a0|                 \u03bcg/m\u00b3|\n|              6|            CO|     Carbon Monoxide|                 \u03bcg/m\u00b3|\n|             25|          PM10|Particulate Matte...|                 \u03bcg/m\u00b3|\n|             26|         PM2.5|Particulate Matte...|                 \u03bcg/m\u00b3|\n|             45|          C6H6|             Benzene|                 \u03bcg/m\u00b3|\n+---------------+--------------+--------------------+----------------------+\n\n"}], "source": "df_Pollutants = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(\"gs://air_quality_cyprus/PollutantsId.csv\")\ndf_Pollutants = df_Pollutants.select(col(\"pollutant_id\"),col(\"pollutant_code\"),col(\"pollutant_name_en\"),col(\"Unit_of_measurement_en\"))\ndf_Pollutants = df_Pollutants.withColumnRenamed('pollutant_id','pollutant_id_pk')\n# Unit_of_measurement_en:\u03bcg/m\u00b3\ndf_Pollutants.show()"}, {"cell_type": "code", "execution_count": 134, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+------------+------------+--------------------+---------------+--------------------+---------------+--------------+-----------------+----------------------+\n|      date|station_code|pollutant_id|mean_pollutant_value|station_code_pk|     station_name_en|pollutant_id_pk|pollutant_code|pollutant_name_en|Unit_of_measurement_en|\n+----------+------------+------------+--------------------+---------------+--------------------+---------------+--------------+-----------------+----------------------+\n|2018-01-29|           1|          45|  2.4291666666666667|              1|Nicosia -Traffic ...|             45|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-05-24|           1|          45|  0.3041666666666667|              1|Nicosia -Traffic ...|             45|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-08-21|           1|          45|               0.325|              1|Nicosia -Traffic ...|             45|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-11-22|           3|          45|                null|              3|Limassol -Traffic...|             45|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-02-27|           8|          45|  0.2916666666666666|              8|Zygi -Industrial ...|             45|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-03-18|           8|          45| 0.16250000000000003|              8|Zygi -Industrial ...|             45|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-05-09|           8|          45|                null|              8|Zygi -Industrial ...|             45|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-12-08|           8|          45|  0.9666666666666668|              8|Zygi -Industrial ...|             45|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-02-12|           9|          45|   0.620833333333333|              9|EMEP- Ayia Marina...|             45|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-06-25|           9|          45| 0.27499999999999997|              9|EMEP- Ayia Marina...|             45|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-09-19|           9|          45| 0.29166666666666663|              9|EMEP- Ayia Marina...|             45|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-11-08|           9|          45|  0.5333333333333333|              9|EMEP- Ayia Marina...|             45|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-06-07|          15|          45| 0.26249999999999996|             15|Pafos - Traffic S...|             45|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-12-31|          14|          45| 0.21666666666666667|             14|Mari - Industrial...|             45|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-02-15|           1|           6|  1148.3521739130433|              1|Nicosia -Traffic ...|              6|            CO|  Carbon Monoxide|                 \u03bcg/m\u00b3|\n|2018-05-21|           1|           6|   402.9095238095238|              1|Nicosia -Traffic ...|              6|            CO|  Carbon Monoxide|                 \u03bcg/m\u00b3|\n|2018-08-21|           1|           6|  439.75652173913056|              1|Nicosia -Traffic ...|              6|            CO|  Carbon Monoxide|                 \u03bcg/m\u00b3|\n|2018-01-08|           3|           6|   948.4208333333332|              3|Limassol -Traffic...|              6|            CO|  Carbon Monoxide|                 \u03bcg/m\u00b3|\n|2018-01-09|           3|           6|   998.7608695652173|              3|Limassol -Traffic...|              6|            CO|  Carbon Monoxide|                 \u03bcg/m\u00b3|\n|2018-03-22|           3|           6|   640.9772727272727|              3|Limassol -Traffic...|              6|            CO|  Carbon Monoxide|                 \u03bcg/m\u00b3|\n+----------+------------+------------+--------------------+---------------+--------------------+---------------+--------------+-----------------+----------------------+\nonly showing top 20 rows\n\n"}], "source": "#Join on aggr data\ndf_aggr_daily = df_aggr_daily.join(df_stations,df_aggr_daily.station_code == df_stations.station_code_pk, how=\"inner\")\ndf_aggr_daily = df_aggr_daily.join(df_Pollutants,df_aggr_daily.pollutant_id == df_Pollutants.pollutant_id_pk, how=\"inner\")\ndf_aggr_daily.show()"}, {"cell_type": "code", "execution_count": 135, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+------------+------------+--------------------+--------------------+--------------+-----------------+----------------------+\n|      date|station_code|pollutant_id|mean_pollutant_value|     station_name_en|pollutant_code|pollutant_name_en|Unit_of_measurement_en|\n+----------+------------+------------+--------------------+--------------------+--------------+-----------------+----------------------+\n|2018-01-29|           1|          45|  2.4291666666666667|Nicosia -Traffic ...|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-05-24|           1|          45|  0.3041666666666667|Nicosia -Traffic ...|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-08-21|           1|          45|               0.325|Nicosia -Traffic ...|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-11-22|           3|          45|                null|Limassol -Traffic...|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-02-27|           8|          45|  0.2916666666666666|Zygi -Industrial ...|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-03-18|           8|          45| 0.16250000000000003|Zygi -Industrial ...|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-05-09|           8|          45|                null|Zygi -Industrial ...|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-12-08|           8|          45|  0.9666666666666668|Zygi -Industrial ...|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-02-12|           9|          45|   0.620833333333333|EMEP- Ayia Marina...|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-06-25|           9|          45| 0.27499999999999997|EMEP- Ayia Marina...|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-09-19|           9|          45| 0.29166666666666663|EMEP- Ayia Marina...|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-11-08|           9|          45|  0.5333333333333333|EMEP- Ayia Marina...|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-06-07|          15|          45| 0.26249999999999996|Pafos - Traffic S...|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-12-31|          14|          45| 0.21666666666666667|Mari - Industrial...|          C6H6|          Benzene|                 \u03bcg/m\u00b3|\n|2018-02-15|           1|           6|  1148.3521739130433|Nicosia -Traffic ...|            CO|  Carbon Monoxide|                 \u03bcg/m\u00b3|\n|2018-05-21|           1|           6|   402.9095238095238|Nicosia -Traffic ...|            CO|  Carbon Monoxide|                 \u03bcg/m\u00b3|\n|2018-08-21|           1|           6|  439.75652173913056|Nicosia -Traffic ...|            CO|  Carbon Monoxide|                 \u03bcg/m\u00b3|\n|2018-01-08|           3|           6|   948.4208333333332|Limassol -Traffic...|            CO|  Carbon Monoxide|                 \u03bcg/m\u00b3|\n|2018-01-09|           3|           6|   998.7608695652173|Limassol -Traffic...|            CO|  Carbon Monoxide|                 \u03bcg/m\u00b3|\n|2018-03-22|           3|           6|   640.9772727272727|Limassol -Traffic...|            CO|  Carbon Monoxide|                 \u03bcg/m\u00b3|\n+----------+------------+------------+--------------------+--------------------+--------------+-----------------+----------------------+\nonly showing top 20 rows\n\n"}], "source": "df_aggr_daily = df_aggr_daily.drop(\"station_code_pk\").drop(\"pollutant_id_pk\")\ndf_aggr_daily.show()"}, {"cell_type": "code", "execution_count": 136, "metadata": {}, "outputs": [{"data": {"text/plain": "[('date', 'string'),\n ('station_code', 'string'),\n ('pollutant_id', 'string'),\n ('mean_pollutant_value', 'double'),\n ('station_name_en', 'string'),\n ('pollutant_code', 'string'),\n ('pollutant_name_en', 'string'),\n ('Unit_of_measurement_en', 'string')]"}, "execution_count": 136, "metadata": {}, "output_type": "execute_result"}], "source": "df_aggr_daily.dtypes"}, {"cell_type": "code", "execution_count": 137, "metadata": {}, "outputs": [], "source": "#Join on aggr data\ndf_aggr_daily.write.option(\"delimiter\", \",\").option(\"quote\", \"\\\"\").option(\"header\", True).mode('overwrite').csv('gs://air_quality_cyprus/Data_Daily_All')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 49, "metadata": {}, "outputs": [{"ename": "AnalysisException", "evalue": "'path hdfs://cluster-spark-m/user/root/data.parquet already exists.;'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o571.parquet.\n: org.apache.spark.sql.AnalysisException: path hdfs://cluster-spark-m/user/root/data.parquet already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:136)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:160)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:157)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:310)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:291)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:586)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n", "\nDuring handling of the above exception, another exception occurred:\n", "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)", "\u001b[0;32m<ipython-input-49-ea3c22f03bb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# DataFrames can be saved as Parquet files, maintaining the schema information.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_aggr_daily\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Read in the Parquet file created above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Parquet files are self-describing so the schema is preserved.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    845\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mAnalysisException\u001b[0m: 'path hdfs://cluster-spark-m/user/root/data.parquet already exists.;'"]}], "source": "# DataFrames can be saved as Parquet files, maintaining the schema information.\ndf_aggr_daily.write.parquet(\"data.parquet\")\n\n# Read in the Parquet file created above.\n# Parquet files are self-describing so the schema is preserved.\n# The result of loading a parquet file is also a DataFrame.\nparquetFile = spark.read.parquet(\"data.parquet\")"}, {"cell_type": "code", "execution_count": 51, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------+------------+-------------------------+\n|pollutant_id|station_code|avg(mean_pollutant_value)|\n+------------+------------+-------------------------+\n|           4|           1|        2.988424547892828|\n|           4|           2|       2.1918487950008325|\n|           3|           8|       13.187183251701633|\n|           5|           1|         58.9167301327806|\n|          45|           8|      0.36437009400526466|\n|           2|           8|       10.344052205001752|\n|           5|           5|        63.89023775298963|\n|           1|           1|       15.883170939337798|\n|           2|           3|       30.227018379703765|\n|           4|           8|       1.5184367062635398|\n|           4|          15|       1.4511682767721918|\n|          45|          14|        0.159159962067982|\n|           5|          16|        70.14754406535972|\n|          45|           5|       0.9853151255101394|\n|           1|          15|        3.901546972253486|\n|           4|          16|       2.3461276196147884|\n|          45|           9|       0.3737103220445582|\n|           3|           3|       46.745757402707376|\n|           2|           2|       22.084769488227074|\n|           6|          15|       297.16524673539425|\n+------------+------------+-------------------------+\nonly showing top 20 rows\n\n"}], "source": "# Parquet files can also be used to create a temporary view and then used in SQL statements.\nparquetFile.createOrReplaceTempView(\"parquetFile\")\nstation_values = spark.sql(\"SELECT pollutant_id,station_code, avg(mean_pollutant_value)  FROM parquetFile Group by pollutant_id,station_code\")\nstation_values.show()"}, {"cell_type": "code", "execution_count": 36, "metadata": {}, "outputs": [{"data": {"text/plain": "[('date', 'string'),\n ('station_code', 'string'),\n ('pollutant_id', 'string'),\n ('avg(pollutant_value)', 'double')]"}, "execution_count": 36, "metadata": {}, "output_type": "execute_result"}], "source": "df_aggr_daily.dtypes"}, {"cell_type": "code", "execution_count": 52, "metadata": {}, "outputs": [], "source": "df_aggr_daily.write.option(\"delimiter\", \",\").option(\"header\", True).mode('overwrite').csv('gs://air_quality_cyprus/Data_Daily_All') "}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 85, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------+------------+------------+---------------+-------------------+----+\n|date_time       |station_code|pollutant_id|pollutant_value|date_type          |hour|\n+----------------+------------+------------+---------------+-------------------+----+\n|01/01/2018 0:00 |1           |45          |4.8            |2018-01-01 00:00:00|00  |\n|01/01/2018 1:00 |1           |45          |5.3            |2018-01-01 01:00:00|01  |\n|01/01/2018 2:00 |1           |45          |6.1            |2018-01-01 02:00:00|02  |\n|01/01/2018 3:00 |1           |45          |4.0            |2018-01-01 03:00:00|03  |\n|01/01/2018 4:00 |1           |45          |3.7            |2018-01-01 04:00:00|04  |\n|01/01/2018 5:00 |1           |45          |1.2            |2018-01-01 05:00:00|05  |\n|01/01/2018 6:00 |1           |45          |1.0            |2018-01-01 06:00:00|06  |\n|01/01/2018 7:00 |1           |45          |1.1            |2018-01-01 07:00:00|07  |\n|01/01/2018 8:00 |1           |45          |1.9            |2018-01-01 08:00:00|08  |\n|01/01/2018 9:00 |1           |45          |0.9            |2018-01-01 09:00:00|09  |\n|01/01/2018 10:00|1           |45          |0.4            |2018-01-01 10:00:00|10  |\n|01/01/2018 11:00|1           |45          |0.3            |2018-01-01 11:00:00|11  |\n|01/01/2018 12:00|1           |45          |0.5            |2018-01-01 12:00:00|12  |\n|01/01/2018 13:00|1           |45          |0.3            |2018-01-01 13:00:00|13  |\n|01/01/2018 14:00|1           |45          |0.3            |2018-01-01 14:00:00|14  |\n|01/01/2018 15:00|1           |45          |0.4            |2018-01-01 15:00:00|15  |\n|01/01/2018 16:00|1           |45          |0.3            |2018-01-01 16:00:00|16  |\n|01/01/2018 17:00|1           |45          |0.3            |2018-01-01 17:00:00|17  |\n|01/01/2018 18:00|1           |45          |0.3            |2018-01-01 18:00:00|18  |\n|01/01/2018 19:00|1           |45          |0.6            |2018-01-01 19:00:00|19  |\n+----------------+------------+------------+---------------+-------------------+----+\nonly showing top 20 rows\n\n"}], "source": "#union https://stackoverflow.com/questions/37332434/concatenate-two-pyspark-dataframes\n#https://sparkbyexamples.com/pyspark/pyspark-to_date-convert-timestamp-to-date/#:~:text=PySpark%20timestamp%20(%20TimestampType%20)%20consists%20of,to%20date%20on%20DataFrame%20column.\n#https://sparkbyexamples.com/pyspark/pyspark-date_format-convert-date-to-string-format/"}, {"cell_type": "markdown", "metadata": {}, "source": "Read "}, {"cell_type": "code", "execution_count": 55, "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[date: string, station_code: string, pollutant_id: string, mean_pollutant_value: string]"}, "execution_count": 55, "metadata": {}, "output_type": "execute_result"}], "source": "df_readback = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(\"gs://air_quality_cyprus/Data_Daily_All\")\ndf_readback                                                                  "}, {"cell_type": "code", "execution_count": 56, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+------------+------------+--------------------+\n|      date|station_code|pollutant_id|mean_pollutant_value|\n+----------+------------+------------+--------------------+\n|2018-03-11|           1|          45|  0.8571428571428571|\n|2018-01-28|           3|          45|   2.895454545454545|\n|2018-02-01|           3|          45|                null|\n|2018-06-30|           3|          45| 0.40833333333333327|\n|2018-08-09|           3|          45|                null|\n|2018-10-09|           3|          45|                null|\n|2018-12-02|           3|          45|                null|\n|2018-12-11|           3|          45|                null|\n|2018-01-24|           5|          45|  1.6124999999999998|\n|2018-02-01|           9|          45|  0.7041666666666665|\n|2018-09-14|          15|          45| 0.25833333333333325|\n|2018-10-02|          15|          45| 0.36956521739130443|\n|2018-02-03|          14|          45|                null|\n|2018-02-22|          14|          45|                null|\n|2018-02-07|           1|           6|   887.5173913043479|\n|2018-08-02|           2|           6|  331.42916666666673|\n|2018-08-03|           2|           6|  295.68750000000006|\n|2018-11-23|           2|           6|  450.75000000000006|\n|2018-03-31|           3|           6|            358.6375|\n|2018-01-26|           5|           6|  479.16666666666674|\n+----------+------------+------------+--------------------+\nonly showing top 20 rows\n\n"}], "source": "df_readback.show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 57, "metadata": {}, "outputs": [], "source": "pd_df_aggr_daily  = df_aggr_daily.toPandas()"}, {"cell_type": "code", "execution_count": 67, "metadata": {}, "outputs": [{"data": {"text/plain": "date                    0\nstation_code            0\npollutant_id            0\nmean_pollutant_value    0\ndtype: int64"}, "execution_count": 67, "metadata": {}, "output_type": "execute_result"}], "source": "pd_df_aggr_daily[pd_df_aggr_daily.isna()==True].count()"}, {"cell_type": "code", "execution_count": 69, "metadata": {}, "outputs": [{"data": {"text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x7f1086da0d10>"}, "execution_count": 69, "metadata": {}, "output_type": "execute_result"}], "source": "pd_df_aggr_daily.plot()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 2}